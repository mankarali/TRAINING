{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e6ddbb-52d3-4a02-9b69-100a4181e3b0",
   "metadata": {},
   "source": [
    "# DS107 Big Data : Lesson One Companion Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd4bfc-9b15-4b80-87b4-b8b60eb56371",
   "metadata": {},
   "source": [
    "### Table of Contents <a class=\"anchor\" id=\"DS107L1_toc\"></a>\n",
    "\n",
    "* [Table of Contents](#DS107L1_toc)\n",
    "    * [Page 1 - Module Overview](#DS107L1_page_1)\n",
    "    * [Page 2 - What is Big Data?](#DS107L1_page_2)\n",
    "    * [Page 3 - Introduction to Amazon Web Services](#DS107L1_page_3)\n",
    "    * [Page 4 - Introduction to Hadoop](#DS107L1_page_4)\n",
    "    * [Page 5 - The Hadoop Ecosystem](#DS107L1_page_5)\n",
    "    * [Page 6 - Key Terms](#DS107L1_page_6)\n",
    "    * [Page 7 - Lesson 1 Hands-On](#DS107L1_page_7)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a8c65-ed48-4354-a257-e01b0fd733d4",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 1 - Module Overview<a class=\"anchor\" id=\"DS107L1_page_1\"></a>\n",
    "\n",
    "[Back to Top](#DS107L1_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e7af81-ed31-4e50-b3f6-8584674d3502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"480\"\n",
       "            src=\"https://player.vimeo.com/video/251886621\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x2295a923820>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "# Tutorial Video Name: Introduction to Big Data\n",
    "VimeoVideo('251886621', width=720, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ad526-95fa-43c4-bc43-1fe807708ed3",
   "metadata": {},
   "source": [
    "The transcript for the above overview video **[is located here](https://repo.exeterlms.com/documents/V2/DataScience/Video-Transcripts/DSO107L01overview.zip)**.\n",
    "\n",
    "# Module Overview\n",
    "\n",
    "Big data refers to both the storage and analysis of large collections of data. For perspective, in the year 2013, **[90% of the world's data](https://www.sciencedaily.com/releases/2013/05/130522085217.htm)** was generated in the previous two years. This trend is going to continue to increase exponentially as computing becomes cheaper. In this module, you will learn how to process large datasets on your local machine, how to distribute work to multiple computers for better performance, and how to prevent and recover from errors. By the end of this module, you should be able to:\n",
    "\n",
    "* Learn the fundamentals of Big Data\n",
    "* Utilize MapReduce in Python\n",
    "* Learn how to maximize computing efficiency\n",
    "* Learn how Hadoop can be used to solve Big Data problems\n",
    "* Get your feet wet with cloud computing by spinning up a virtual machine with AWS\n",
    "* Use PySpark to analyze large data\n",
    "* Learn how to recover from data processing errors\n",
    "\n",
    "---\n",
    "\n",
    "## Lesson Overview\n",
    "\n",
    "In this lesson, you will get an introduction to big data and the Hadoop ecosystem.\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Additional Info!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>You may want to watch this <a href=\"https://vimeo.com/456037958\"> recorded live workshop on the concepts in this lesson. </a> </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990ac7a-5a2f-4744-af14-1b7e385ea484",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 2 - What is Big Data?<a class=\"anchor\" id=\"DS107L1_page_2\"></a>\n",
    "\n",
    "[Back to Top](#DS107L1_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612429df-e4ef-4d85-8460-5bc8914079c3",
   "metadata": {},
   "source": [
    "# What is Big Data?\n",
    "\n",
    "Data is usually categorized as \"big\" when you can no longer process it efficiently on one computer.  However, the parameters for this will differ by both dataset and computer setup.  There is no one cutoff that allows someone to say \"my data is big;\" there is no set number of columns or rows that will push you over the edge into \"big data\" territory.  You could have a relatively small number of records (rows), but have a lot of columns.  You could have data that are images, audio, or text that is space and processing intensive.  You could have a relatively small set of variables in key-pair format that is updated incredibly frequently.  All of these may or may not be instances of big data, based on their particular ramifications AND based on the computer you're using.  A ten year old system running an i5 processor is going to be able to handle much less data than a brand-new computer with a solid state hard drive and an i10 processor.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "If ACME Groceries has collected both the dates and times of each item sold in their store, they're empowered to ask questions concerning inventory. This sounds simple, right? The variables you're interested in are only date, time, and item.  How can that possibly become big data? Well, think about how many items a store has.  Think about how many customers PER DAY they may receive.  Think about how many items at a time a customer may purchase.  Is it starting to look a bit bigger now? What if you wanted just a little more information, and you increased data collection to include multiple ACME Groceries in the chain? Now maybe you need store location as well, and you have increased the number of dates, times, and items tenfold. The amount of data generated and stored becomes monumental! \n",
    "\n",
    "With fine detail and vast records of customer purchase history, companies can spot patterns and predict customer behavior. Big data makes large-scale insights possible. For instance, consider the following questions that can be answered with just those few variables and the actions that can be taken with those answers:\n",
    "\n",
    "* What month has the highest produce sales? \n",
    "  * _Action:_ Mail out advertisements and coupons the week prior to additionally incentivize produce purchase.\n",
    "* Which two items are frequently purchased together?\n",
    "  * _Action:_ Place these items adjacent to each other to enhance the likelihood they will both be purchased.\n",
    "* What are the five most frequently purchased items between the hours of 6:00 AM and 8:00 AM?\n",
    "  * _Action:_ Place the items in a popup kiosk near the front of the store for quick access in the morning.\n",
    "* Which brand of toothpaste generates the most money?\n",
    "  * _Action:_ Move the best seller to the middle shelf (eye-level) to encourage purchases.\n",
    "\n",
    "---\n",
    "\n",
    "## Properties of Big Data\n",
    "\n",
    "The properties of big data are all denoted with a V. There are the main 3 Vs of Big Data, which are _Volume_, _Variety_, and _Velocity_.\n",
    "\n",
    "1.  **Volume** refers to the size of the datasets being stored. According to an **[article](https://www.northeastern.edu/graduate/blog/how-much-data-produced-every-day/)** by Northeastern University, the total amount of data in the world is projected to rise steeply to 44 zetabytes by 2020. For those who are unfamiliar with the **[metric prefixes](https://www.nist.gov/pml/weights-and-measures/metric-si-prefixes)**, 44 zetabytes expanded is 44,000,000,000,000,000,000,000 bytes! \n",
    "\n",
    "    <div class=\"panel panel-success\">\n",
    "        <div class=\"panel-heading\">\n",
    "            <h3 class=\"panel-title\">Additional Info!</h3>\n",
    "        </div>\n",
    "        <div class=\"panel-body\">\n",
    "            <p>To really get a perspective on how much data volume has changed, think about this: NASA traveled to the moon and back on only 4kb of memory! For more details about the early Apollo computing system, <a href=\"https://www.metroweekly.com/2014/07/to-the-moon-and-back-on-4kb-of-memory/\"> check this article out! </a></p>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "2.  **Variety** refers to the different forms the data can take, such as video, audio, and text. These types of data often take more space to store and require longer processing times as well. Some systems will have a lot of variety, but others may be a little less dispersed. For example, Twitter's _variety_ dimension may consist mostly of text while YouTube's _variety_ dimension is primarily video.\n",
    "\n",
    "3.  **Velocity** refers to the speed of data generation and storage. This typically ties into the \"real-time\" rate of data transmission.  With large-scale or global companies that are dealing with transactional or monitoring data, the velocity can be incredibly fast.  For instance, **[back in 2013 there were 143,199 Tweets being sent per second](https://blog.twitter.com/engineering/en_us/a/2013/new-tweets-per-second-record-and-how.html)**. \n",
    "\n",
    "In addition, as big data has gotten more popular, other Vs have been added.  Consider also noting: \n",
    "\n",
    "4. **Veracity:** How trustworthy the data is. As data is coming in, is it likely to create discrepancies on your cluster? Are you likely to have duplicate records? What happens in the case of computer failure?\n",
    "\n",
    "5. **Value:** It don't mean a thing if it ain't got that swing! Big data has got to provide useful information and insights from it, otherwise, why are you throwing all this computer processing power and storage at it?\n",
    "\n",
    "---\n",
    "\n",
    "## Big Data Computing\n",
    "\n",
    "A fundamental part of big data analysis is being able to process all of the necessary data, and because of the nature of big data, you're going to need more than one computer, elsewise the the processing time and cost is too much to justify the usefulness of the data. What if the work could be shared among two computers? Well, that would cut the time in half. What if a third computer were added? As you may guess, the more computers that are added, the shorter the amount of time required to process the same amount of data. This is called *scalability* - the ability to keep throwing additional computers at the data in a linear fashion - the more data you have, the more computers you need.\n",
    "\n",
    "When you're harnessing multiple computers to do one job, altogether those computers are referred to as a *cluster*. Each individual computer in a cluster is referred to as a *node* typically, and you will have one main node that is running the whole shebang, and then others that are just doing the work.  The node that is in charge is called the *master node*, or sometimes the *manager*, and the computers that are working are the *slave nodes* or sometimes the *workers*.   \n",
    "\n",
    "![A laptop labeled Master Manager has one vertical and two horizontal lines in between connects six laptops, three in row one and three in row two.](Media/bigData1.png)\n",
    "\n",
    "Note that your workplace may or may not actually physically own the computers in the cluster. There are many services out there that allow you to rent out clusters.  This then becomes an endeavor in *virtual machines* as well.  For instance, depending on the processing needs of your data, you may be renting anywhere from a small partition on just one of the small silver boxes below to several silver boxes (each a computer). \n",
    "\n",
    "![A server room.](Media/bigData2.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832358fd-55fa-420a-8aa2-46fbf2097986",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 3 - Introduction to Amazon Web Services<a class=\"anchor\" id=\"DS107L1_page_3\"></a>\n",
    "\n",
    "[Back to Top](#DS107L1_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564e3e7-3a7f-4936-b3a3-2e0a9ab78512",
   "metadata": {},
   "source": [
    "# Introduction to Amazon Web Services\n",
    "\n",
    "There are many different ways and places from which to spin up virtual machines and rent clusters. One of them is *Amazon Web Services (AWS)*, which you will be utilizing later in this module.  In order to access AWS for free, and to take advantaged of their free student services, like training and job boards, you will create an AWS Educate account.  Below are the directions to create this account. \n",
    "\n",
    "---\n",
    "\n",
    "## What is AWS?\n",
    "\n",
    "From Amazon's website:\n",
    "\n",
    "> \"Amazon Web Services (AWS) is a secure cloud services platform, offering compute power, database storage, content delivery and other functionality to help businesses scale and grow. Explore how millions of customers are currently leveraging AWS cloud products and solutions to build sophisticated applications with increased flexibility, scalability and reliability.\" - [Amazon AWS](https://aws.amazon.com/what-is-aws/).\n",
    "\n",
    "The process to obtain an AWS account is described below.\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Additional Info!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>If you already have an AWS Educate account, you can skip to the end of the page.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "# Amazon Web Services (AWS) Account\n",
    "\n",
    "You will need an AWS account to proceed with this course and deploy web apps. The process to obtain an AWS account is described below.\n",
    "\n",
    "**If you already have an AWS account or already applied for an AWS Educate account, you can skip this section and head to the section on accessing your AWS Educate account.**\n",
    "\n",
    "AWS Educate is a program Amazon offers that will allow you to obtain an AWS account for free (with limitations). To apply for an account, click the link below:\n",
    "\n",
    "**[AWS Educate](https://aws.amazon.com/education/awseducate/apply/)**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1 - Check Email for AWS Educate Application Process\n",
    "You should receive an email from AWS Educate Support providing a link to complete the `AWS Educate application process`.   **If you did not receive an email, please inform your mentor right away**.\n",
    "\n",
    "![A snapshot of an email from AWS Educate Support where four options and one icon are highlighted in blue, below salutations and email content is mentioned.](Media/sign-up_email-one.png \"AWS Educate Application Process Step 1\")\n",
    "---\n",
    "\n",
    "## Step 2 - Fill In Your Details\n",
    "\n",
    "Once you click the `here` option, you should be taken to the next page to enter your details. You should see something similar to the screenshot below:\n",
    "\n",
    "![A snapshot of a page from AWS educate displaying the second step for the registration process. There are ten text fields labeled school or institution name, first name, email, birth month, birth year, country, last name, graduation month, graduation year, and promo code. A checkbox labeled I'm not a robot is placed below the text fields. A button labeled next is also placed at the bottom of the page.](Media/signup-details.png \"AWS Educate Apply Step 2\")\n",
    "\n",
    "You need to enter `Woz U Education Holdings, LLC` into the __Institution Name__ field. As you begin typing, it should begin to filter the list of institutions, giving you the option to select it from the list.\n",
    "\n",
    "You'll also need to enter the __country__, __city__, and __state__ of the institution, which are the next three fields near the institution name:\n",
    "\n",
    "* Country: **United States**\n",
    "* City: **Phoenix**\n",
    "* State: **Arizona**\n",
    "\n",
    "Once you've entered the above values fill in the remainder of the fields with the values that are appropriate for you. All fields will require a value in order to move to the next page. Please ensure first name, last name, birthday, graduation month and year are all filled.  __Do not enter a promo code__.\n",
    "\n",
    "You may also need to solve a CAPTCHA.\n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>It is very important that you choose the correct institution! Make sure you select Woz U Education Holdings, LLC.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Click \"NEXT\" to proceed.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3 - Check for Confirmation Email \n",
    "\n",
    "Once you have completed filling out your account details, AWS Educate Support will send a __\"Thank you for submitting your AWS Educate Account application\"__ email. \n",
    "\n",
    "![A snapshot of an email from AWS Educate Support where four options and one icon are highlighted in blue, below salutations and email content with a link is mentioned. On the bottom right, a rectangular box is captioned.](Media/signup-confirmation.png \"AWS Educate Apply Step 3\")\n",
    "\n",
    "Your email should look similar to the image above. Once you've accessed the email, click on the link provided to confirm your email address and complete the application process. Once you click the link provided in the email, you will see the following screen. \n",
    "\n",
    "![A snapshot of a website labeled AWS educate below two blue backgrounds divided is captioned. In the first blue background, a rectangular box is captioned.](Media/signup-verified.png \"AWS Educate Apply Step 4\")\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4 - Set AWS Password \n",
    "\n",
    "Once your application has been verified, you will receive the following email to set your password. \n",
    "\n",
    "![A snapshot of an email from AWS Educate Support, where four options and one icon are highlighted in blue, below salutations and email content, links are mentioned, where Click here is highlighted in yellow.](Media/signup-password.png \"AWS Educate Apply Step 5\")\n",
    "\n",
    "Select the `Click here` option to set your account password. \n",
    "\n",
    "![A snapshot of a website labeled AWS educate below captioned has two boxes with eight dots present in each. A yellow button labeled Set Password, In bold five roman letters mentioned in points are captioned. The fifth roman letter contains ten special characters.](Media/signup-createpassword.png \"AWS Educate Apply Step 6\")\n",
    "\n",
    "Once you click `Set Password` you will be redirected to your AWS account, shown in the below. \n",
    "\n",
    "---\n",
    "\n",
    "## Step 5 - Access AWS Account \n",
    "\n",
    "![A window labeled AWS educate on the right six options is present where a red arrow points to the fifth option which is highlighted in yellow, below three boxes are captioned where the third box header is highlighted in blue and below a blue button is present.](Media/signup-account.png \"AWS Educate Apply Step 7\")\n",
    "\n",
    "Click `AWS Account` in the upper right corner of your dashboard. You will be directed to another page where you will be given the option to `Create Starter Account`\n",
    "\n",
    "![A window labeled AWS educate on the left corner, on the right corner six options are present where the fifth option is underlined in blue. Below on the left portrays a man carries a laptop in the air, on the right header captioned in yellow and the rest in black, below a yellow button is present in a black circle.](Media/signup-starterAcct.png \"AWS Educate Apply Step 8\")\n",
    "\n",
    "You will then be redirected to use your AWS Educate Starter Account to access the AWS Console and resources to begin building. \n",
    "\n",
    "![A window labeled AWS educate on the left corner, on the right corner six options are present. Below on the left portrays a man carries a laptop in the air, on the right header captioned in yellow and the rest in black, in between a yellow button is present in a red circle.](Media/signup-console.png \"AWS Educate Apply Step 9\")\n",
    "\n",
    "Once you have clicked the `AWS Educate Starter Account` button, you will be able to view your AWS Account Status, where you will want to select `AWS Console` option. Finally, you will be directed to your AWS Management Console.\n",
    "\n",
    "![A web page labeled AWS management console displays a text field labeled find services. The web page is divided into five panels. The first panel is labeled as build a solution, the second panel is labeled as learn to build, the third panel is labeled stay connected to your AWS resources, the fourth panel is labeled as explore AWS, and the fifth panel is labeled as have a feedback.](Media/signup-management.png \"AWS Educate Apply Step 10\")\n",
    "\n",
    "The AWS Console is what you'll need for the course work. There will be additional material on how to use the AWS Console later in the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aade2-59af-4cbc-b5ed-da31e39f266d",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 4 - Introduction to Hadoop<a class=\"anchor\" id=\"DS107L1_page_4\"></a>\n",
    "\n",
    "[Back to Top](#DS107L1_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2ed6e-98dd-43f7-8761-6ecc9a13af08",
   "metadata": {},
   "source": [
    "# Introduction to Hadoop\n",
    "\n",
    "The system to know in the world of big data is *Hadoop*.  Hadoop is, at its heart, a way to distribute and manipulate data across a cluster.  What separates Hadoop from its predecessors is the ability to interact with your cluster as if it was a single computer; Hadoop does most of the distribution work behind the scenes so you don't have to worry about it. Hadoop is not the hardware of the big data; the machines you are using in your cluster can come from anywhere.  Instead, it is the software that allows your data files to get broken up, stored, accessed, and used seamlessly. \n",
    "\n",
    "![Logo of Apache Hadoop](Media/bigData3.png)\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Fun Fact!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Hadoop is named after the developer's child's stuffed yellow elephant. Knowing this, does the logo make more sense? </p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Fault Tolerance\n",
    "\n",
    "One of the most important things about big data in general, and Hadoop in particular, is that failure recovery is key.  If any part of the cluster goes down, you don't want to lose access to that data, and any processing you've done with it! Think about the devastating effects of running out of computer battery and not being able to save your homework assignment, and the hours it might take you to re-do something.  Then think about what would happen if an error like that took place on one node of a cluster, and it brought down all the others! You might lose billions of records and thousands of hours worth of work.  A catastrophic loss like that is obviously out of the question, so when working with big data it's important to be able to not only have a backup, which is called *redundancy*, but also to keep the work going even if you lose one node.  This ability to keep going even when things go to heck in a handbasket is called *fault tolerance*. Hadoop automatically keeps a backup of your work, and even further, has built-in programming so that if one node goes down, other nodes can be added or utilized more fully to keep your jobs running smoothly. \n",
    "\n",
    "---\n",
    "\n",
    "## Core Hadoop Components\n",
    "\n",
    "There are three components that go into the main Hadoop system, although many other programs have now been built to sit upon and work with these main components.  These three main components in Hadoop are HDFS, YARN, and MapReduce: \n",
    "\n",
    "![Three boxes present, the first box labeled MapReduce, the second box labeled YARN, and the third box labeled HDFS.](Media/bigData4.png)\n",
    "\n",
    "* **Hadoop Distributed File System (HDFS):** HDFS is the start of it all! HDFS allows you to distribute data storage across the cluster, provides a backup system, and makes your cluster look and feel like one computer with which you can interact.\n",
    "\n",
    "* **Yet Another Resource Negotiator (YARN):** YARN sits on top of HDFS and manages the resources in your cluster. In big data, *resources* typically refers to node processing power, so it is YARN's job to determine what nodes are available for extra work and to keep your cluster up and running.\n",
    "\n",
    "* **MapReduce:** MapReduce is a programming model that allows you to process your data across the entire cluster.  These days, it is old and slow, so you'll mainly interact with other programs that are harnessing the power of MapReduce instead of actually using MapReduce yourself.  Nevertheless, the function of MapReduce allows for all the distributed analytics you will run.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4ad3c-2fe6-457e-8924-1410e951bebd",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 5 - The Hadoop Ecosystem<a class=\"anchor\" id=\"DS107L1_page_5\"></a>\n",
    "\n",
    "[Back to Top](#DS107L1_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc06a2-e702-47a1-942f-cbcbe31221a7",
   "metadata": {},
   "source": [
    "# The Hadoop Ecosystem\n",
    "\n",
    "With those Hadoop components at the core, other programs can work on top of them and/or connect with them to increase processing speed, functionality, or data availability. The wide world of programs that interact with Hadoop in some way is known as the *Hadoop ecosystem*. Here is an image of the complete Hadoop ecosystem: \n",
    "\n",
    "![Three boxes are labeled as MySQL, Cassandra, and mongoDB and they are grouped as external data storage. A table titled Ambari. The fields of the table are labeled zookeeper, oozie, pig, hive, map reduce, TEZ, Spark, HBase, Storm, YARN, Mesos, HDFS, and data ingestion. Another set of boxes are labeled drill, phoenix, hue, presto, and zeppelin are grouped and labeled query engines.](Media/bigData6.png)\n",
    "\n",
    "Now that you have a broad idea of all the pieces and technologies possible, you will learn a little about each and where they fit.  Keep in mind that this module will not cover every single component listed here, as many are redundant.  However, you will get a high-level overview of the systems so that you will be able to recognize them and their function should you encounter them outside this course.\n",
    "\n",
    "You'll ingest this diagram starting at the center and working your way up and out. \n",
    "\n",
    "---\n",
    "\n",
    "##  Analytics Programs\n",
    "\n",
    "Sitting on top of HDFS is both YARN and Mesos.  \n",
    "\n",
    "* **Mesos:** Mesos is an alternative to YARN. It's function is nearly identical to YARN as a resource negotiator.\n",
    "\n",
    "Sitting on top of YARN is TEZ.\n",
    "\n",
    "* **TEZ:** TEZ speeds up the MapReduce capabilities considerably, and can be used in conjunction with Hive and Pig for faster processing.\n",
    "\n",
    "Sitting on top of either YARN or Mesos: \n",
    "\n",
    "* **Spark:** Apache Spark uses the basics of MapReduce but takes it much further.  It is used for data querying, analysis, machine learning, and streaming in real-time.  You can write into Spark using Python, Scala, or Java. \n",
    "\n",
    "There are two programs that sit on top of MapReduce: \n",
    "\n",
    "* **Pig:** Pig is a SQL-like query program that runs off MapReduce and has the option of using TEZ.  \n",
    "\n",
    "    <div class=\"panel panel-success\">\n",
    "        <div class=\"panel-heading\">\n",
    "            <h3 class=\"panel-title\">Fun Fact!</h3>\n",
    "        </div>\n",
    "        <div class=\"panel-body\">\n",
    "            <p>The official language that Pig uses is Pig Latin! Not a joke!</p>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "* **Hive:** Hive allows you to make SQL queries against a database stored on your cluster. It also has the option of using TEZ to improve processing speed.\n",
    "\n",
    "---\n",
    "\n",
    "## Built-in Transactional Database\n",
    "\n",
    "On top of HDFS, there is HBase. \n",
    "\n",
    "* **HBase:** HBase is a transactional database using key-value pairs (think NoSQL!) that is built into Hadoop.  It can provide fast results to query systems like Spark, Pig, and Hive.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-time Data Processing Program\n",
    "\n",
    "Alongside this analysis setup to the right is Storm and programs used for *data ingestion*, or getting data into your cluster.\n",
    "\n",
    "* **Storm:** Apache Storm allows you to process data in real-time as it comes in. You might use this when processing sensor data, which often comes in by the second.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Ingestion Programs\n",
    "\n",
    "* **Sqoop:** Sqoop is a data ingestion program that allows you to tie in legacy SQL databases such as a MySQL Oracle setup via a *database connection (DBC)*. \n",
    "\n",
    "* **Flume:** Flume is a data ingestion program that can transport web logs into your cluster in real time.\n",
    "\n",
    "* **Kafka:** Kafka is a broad data ingestion program that can collect data of any sort from a cluster of computers and broadcast them to your Hadoop cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## Cluster Management Programs\n",
    "\n",
    "To the left of the analysis setup are two programs that help you manage and maintain your Hadoop cluster: Oozie and Zookeeper.\n",
    "\n",
    "* **Oozie:** Oozie allows you to schedule jobs on your cluster, and integrates with other programs in your Hadoop ecosystem so that they can be passed from one to another with ease.\n",
    "\n",
    "* **Zookeeper:** Zoookeeper helps you coordinate your cluster's nodes and allows you to maintain a stable system even if your master node goes down. It's primary use is for failure recovery.\n",
    "\n",
    "---\n",
    "\n",
    "## External Data Storage Programs\n",
    "\n",
    "There are also programs that don't layer within the Hadoop ecosystem but instead reach into it to interact, but primarily stay separate.  There are two overarching groups: programs that function as external storage outside the Hadoop cluster and programs that function as query engines outside the Hadoop Cluster. You'll learn aobut the external data storage programs first: \n",
    "\n",
    "* **MySQL:** Really, this includes MySQL as well as any other external SQL database.  You can import data from a SQL database to process on your cluster, and then export the results back to the database if desired.\n",
    "\n",
    "* **cassandra:** This is a non-relational database (NoSQL) that can sit between your real-time website data and your Hadoop cluster.\n",
    "\n",
    "* **mongoDB:** Another non-relational database you can utilize to connect to the Hadoop cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## Query Engines\n",
    "\n",
    "* **Drill:** Apache Drill is a query system that allows you to conduct SQL queries on non-relational databases such as cassandra and mongoDB.\n",
    "\n",
    "* **Phoenix:** Apache Phoenix allows you to query across a wide range of data storage options and makes them all look and feel relational, even if they are not.\n",
    "\n",
    "* **Hue:** An interactive query system with visualizations that allows you to use Hive and/or HBase.  Primarily used in Cloudera distributions of Hadoop.\n",
    "\n",
    "* **presto:** Another way to execute queries across your entire cluster.\n",
    "\n",
    "* **Zeppelin:** Apache Zeppelin is a notebook approach to querying that is similar in nature to Jupyter Notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## The GUI\n",
    "\n",
    "Overlying all of this is Apache Ambari, which is the graphical user interface for your Hadoop cluster. There will be a lot you can accomplish through Ambari, like adding and deleting files, using Hive and Pig, and turning on and off services and clusters, with some point-and-click or easy coding functionality. Though you will still need to use the command line at times, Apache Ambari is a lifesaver!\n",
    "\n",
    "You will go into these with much more detail at a later date, though again not everything will be covered, as there is quite a bit of redundancy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e3b8a-9224-447f-8cf7-5035cae86421",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 6 - Key Terms<a class=\"anchor\" id=\"DS107L1_page_6\"></a>\n",
    "\n",
    "[Back to Top](#DS107L1_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e2098-28ef-4ff5-a155-afe824de4323",
   "metadata": {},
   "source": [
    "# Key Terms\n",
    "\n",
    "Below is a list and short description of the important keywords learned in this lesson. Please read through and go back and review any concepts you do not fully understand. Great Work!\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Cluster</td>\n",
    "        <td>A group of connected computers.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>3 Vs of Big Data</td>\n",
    "        <td>Volume, variety, and velocity.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Volume</td>\n",
    "        <td>Size of the data being stored.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Variety</td>\n",
    "        <td>Different types of data.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Velocity</td>\n",
    "        <td>Speed of data generation and storage.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Veracity</td>\n",
    "        <td>Trustworthiness of the data.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Value</td>\n",
    "        <td>Usefulness of the data.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Scalability</td>\n",
    "        <td>The ability to continue adding computers to deal with more data.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Cluster</td>\n",
    "        <td>A group of computers all harnessed for the same purpose.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Node</td>\n",
    "        <td>A computer in the cluster.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Master Node</td>\n",
    "        <td>AKA Manager.  The node controlling the other computers in the cluster.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Slave Node</td>\n",
    "        <td>AKA Worker.  The computers doing the work in the cluster.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Amazon Web Services (AWS)</td>\n",
    "        <td>A system for spinning up virtual machines to use in big data processing.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Hadoop</td>\n",
    "        <td>A program that harnesses a cluster for big data processing but allows the user to feel like they're interacting with one computer instead of many.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Redundancy</td>\n",
    "        <td>Ensuring data backup.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Fault Tolerance</td>\n",
    "        <td>Ability to continue processing even after a part of the cluster has crashed.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Data Ingestion</td>\n",
    "        <td>Getting data into your cluster.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Database Connection (DBC)</td>\n",
    "        <td>A way of connecting to SQL databases.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb7d8a-aced-42e5-ac24-32acf8c95473",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 7 - Lesson 1 Hands-On<a class=\"anchor\" id=\"DS107L1_page_7\"></a>\n",
    "\n",
    "[Back to Top](#DS107L1_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0debaf-5fae-4035-8e7d-ae62e62456cb",
   "metadata": {},
   "source": [
    "# Lesson 1 Hands On\n",
    "\n",
    "This Hands-­On **will** be graded, so make sure you complete each part. When you are done, please submit one document with all of your findings for grading.\n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Do not submit your project until you have completed all requirements, as you will not be able to resubmit.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Pick one aspect of the Hadoop Ecosystem that you are most interested in and read a little further about it. Write down a few interesting key points about that program and how it operates. \n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Be sure to zip and submit your entire directory when finished!</p>\n",
    "    </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
