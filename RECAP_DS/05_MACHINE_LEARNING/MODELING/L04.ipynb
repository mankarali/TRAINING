{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e6ddbb-52d3-4a02-9b69-100a4181e3b0",
   "metadata": {},
   "source": [
    "# DS106 Modeling : Lesson Four Companion Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd4bfc-9b15-4b80-87b4-b8b60eb56371",
   "metadata": {},
   "source": [
    "### Table of Contents <a class=\"anchor\" id=\"DS106L4_toc\"></a>\n",
    "\n",
    "* [Table of Contents](#DS106L4_toc)\n",
    "    * [Page 1 - Introduction](#DS106L4_page_1)\n",
    "    * [Page 2 - Multiple Regression Models](#DS106L4_page_2)\n",
    "    * [Page 3 - Stepwise Regression](#DS106L4_page_3)\n",
    "    * [Page 4 - Backward Elimination](#DS106L4_page_4)\n",
    "    * [Page 5 - Forward Selection](#DS106L4_page_5)\n",
    "    * [Page 6 - Hybrid Stepwise - Forward and Backward Selection](#DS106L4_page_6)\n",
    "    * [Page 7 - Key Terms](#DS106L4_page_7)\n",
    "    * [Page 8 - Lesson 4 Practice Hands-On](#DS106L4_page_8)\n",
    "    * [Page 9 - Lesson 4 Practice Hands-On Solution](#DS106L4_page_9)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a8c65-ed48-4354-a257-e01b0fd733d4",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 1 - Overview of this Module<a class=\"anchor\" id=\"DS106L4_page_1\"></a>\n",
    "\n",
    "[Back to Top](#DS106L4_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a3f1f5-dd00-419a-be39-0382cc40810e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"480\"\n",
       "            src=\"https://player.vimeo.com/video/246121381\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x2883ff597c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "# Tutorial Video Name: Modeling with step-wise regression\n",
    "VimeoVideo('246121381', width=720, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ad526-95fa-43c4-bc43-1fe807708ed3",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "As a data scientist, regularly you will likely be faced with a large dataset and a request by the process owner to \"make sense of the data.\" This is a daunting task. One of the approaches to solving a problem such as this is to employ a technique called *stepwise regression*.\n",
    "\n",
    "You have learned in previous lessons about linear regression, logistic regression, and non-linear regression. In each case, you had a single response variable, and a single predictor variable with which you created a model. Well, you're hitting the big-time now, because now you're adding additional predictors! As many as your data allows! Just adding them in is the concept of *multiple regression*.  But if you want to sort out which ones are most important, than you'll need to employ *stepwise regression*. \n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "\n",
    "* Understand multiple regression\n",
    "* Differentiate between the three different types of stepwise regression\n",
    "* Complete backwards elimination, forward selection, and hybrid stepwise regression in R\n",
    "\n",
    "This lesson will culminate in a hands-on in which you determine the best linear regression model through stepwise regression in R.\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Additional Info!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>You may want to watch this <a href=\"https://vimeo.com/438421794\"> recorded live workshop </a> that goes over stepwise regression in R.</p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990ac7a-5a2f-4744-af14-1b7e385ea484",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 2 - Multiple Regression Models<a class=\"anchor\" id=\"DS106L4_page_2\"></a>\n",
    "\n",
    "[Back to Top](#DS106L4_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612429df-e4ef-4d85-8460-5bc8914079c3",
   "metadata": {},
   "source": [
    "# Multiple Regression Models\n",
    "\n",
    "When you have only one independent and one dependent variable, that is referred to as *simple regression*. But if you want to add in more independent variables, then that is referred to as *multiple regression*. \n",
    "\n",
    "In the real world, there are many things that go into predicting any one outcome, which means that you need to find a way to estimate with multiple predictors (independent variables). For example, if you want to predict the stopping distance for a car on dry roads, you might want to use:\n",
    "\n",
    "* The initial speed of the car\n",
    "* The weight of the car\n",
    "* The amount of tread on the tires\n",
    "\n",
    "The high level theory is that you will add additional predictor variables when they improve the model. In other words, if the ability to predict increases significantly, then you should include those additional predictor variables.\n",
    "\n",
    "For example:\n",
    "\n",
    "* If height is a pretty good predictor of weight, then height **and** BMI might be a better predictor of weight.\n",
    "* If foot width is a pretty good predictor of foot length, then foot width **and** height might be a better predictor of weight.\n",
    "* If high school GPA is a pretty good predictor of college GPA, then high school GPA **and** SAT score might be a better predictor of college GPA.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832358fd-55fa-420a-8aa2-46fbf2097986",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 3 - Stepwise Regression<a class=\"anchor\" id=\"DS106L4_page_3\"></a>\n",
    "\n",
    "[Back to Top](#DS106L4_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564e3e7-3a7f-4936-b3a3-2e0a9ab78512",
   "metadata": {},
   "source": [
    "# Stepwise Regression\n",
    "\n",
    "*Stepwise regression* is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a predictor variable is considered for addition to or subtraction from the set of explanatory variables based on some pre-specified criterion. Usually, this takes the form of a sequence of *F*-tests or *t*-tests. Stepwise regression is a tool that can help determine a complex relationship. Modeling is an iterative process, and stepwise regression certainly has a seat at the table.\n",
    "\n",
    "The term \"stepwise\" comes from the fact that the process is iterative. Each time a term is eliminated, the remaining model is re-evaluated. This is in contrast to determining multiple terms that can be eliminated on the initial observation. The phrase \"...in the presence of the other terms\" is important here, because each term individually will behave differently depending on what other terms are in the model.\n",
    "\n",
    "Suppose you have a dataset that includes a single response variable, and 6 different possible predictor variables. How do you figure out what the best model is? One approach would be to look at all possible models, and simply choosing the best one based on an r<sup>2</sup> value. Seems reasonable, right? The higher the r<sup>2</sup> value is, the more predicting power the model has - at least, that is the theory. However, this approach has a couple of weaknesses that makes it less than ideal: \n",
    "\n",
    "* **This becomes a lot of models very quickly!** The number of possible models is two raised to the power of your predictors: 2<sup>x</sup>. In the case of 6 predictor variables, that would be 2<sup>6</sup> models, or 64 models. You could take the time to look through 64 models to determine which one is best, but it will be very tedious. And that's not even including any interactions between your variables or any terms for non-linear modeling. Can you imagine what would happen if there were 30 predictor variables? That would give 2<sup>30</sup> possible models, but 2<sup>30</sup> ~ 1.07 billion models. You and all your friends could spend 1000 lifetimes looking over the models and doing nothing else, and you would still never find the best one!\n",
    "\n",
    "* **R<sup>2</sup> will never decrease:** As you add predictor terms to a model, R<sup>2</sup> may stay the same, or it might increase, but it will never decrease. In other words, the model that contains **all** of the predictor variables will have the highest r<sup>2</sup> value. If that is your only criteria for determining which model is best, then you would automatically have to use the model that contains all predictor variables.\n",
    "\n",
    "---\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "You might wonder what is wrong with including all the predictor variables. Well, using all of the predictor variables increases the chances of *overfitting* the model. Basically, the model fits your current data so well that it won't fit similar data later. It can't be generalized.\n",
    "\n",
    "There is a thing called the *Principle of Parsimony* in science. It is applied to many branches of science, and in a nutshell, it means that the most acceptable explanation of any phenomenon is usually the simplest explanation. That is not to say that there are no complex solutions to science problems; there certainly are. The more accurate interpretation is that if there are two \"equally\" good models for a particular relationship, whichever is simpler is usually the preferred model.\n",
    "\n",
    "---\n",
    "\n",
    "## Approaches for Stepwise Regression\n",
    "\n",
    "So now that you know what doesn't work, what does? Stepwise regression, of course! There are three different approaches:\n",
    "\n",
    "1. Backward Elimination\n",
    "2. Forward Selection\n",
    "3. Hybrid\n",
    "\n",
    "You will learn about all three in this lesson, with the help of the organization \"Dragonfly Statistics\" and their tutorial videos.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Backward Elimination\n",
    "\n",
    "Start with all the predictors, and then start eliminating one variable at a time, starting with the IV that has the least effect on improving the model. Keep removing terms until the point at which removing a term has a significant detrimental effect on the model, and then stop.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example\n",
    "\n",
    "Suppose you wanted to create a model to predict someone's weight. You all have a pretty good understanding of what contributes to someone's weight. For example, you can all pretty much agree a person's height, gender, body type, body fat percentage, etc. all could help predict a person's weight. Some of those might be better predictors than others, but they all probably have some sort of prediction ability. On the other hand, you might know stuff about a person that probably has little or nothing to do with a person's weight, such as their IQ, the number of cousins they have, or whether or not they like rodeo.\n",
    "\n",
    "Someone who is an expert on heights of humans might be able to toss out the meaningless predictors, state the predictors that are completely meaningful, and even weight the predictors that are marginal. But for the rest of you, how would you know? Using the technique of backward elimination would create a model with all of the predictors in the model, and then it would determine (one at a time) which terms don't belong, based on criteria you will discuss later.\n",
    "\n",
    "The general approach is that if a single predictor - in the presence of all other predictors - does not add to the quality of the model, then it is dropped. Each time a predictor is dropped, the new model is evaluated to determine if there is another predictor can be dropped. \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Forward Selection\n",
    "\n",
    "Start with the single predictor that has the greatest prediction power in the model, and then add predictors one at a time, beginning with the remaining predictor that has the greatest prediction power in the presence of all the terms already included in the model. Once you get to a point where adding an additional term has little effect in improving the predictability of the model, stop.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Hybrid: Combining Forward Selection and Backward Elimination\n",
    "\n",
    "This method combines the first two methods. At each step, the method determines if a term needs to be added, and if any of the terms previously added need to be eliminated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aade2-59af-4cbc-b5ed-da31e39f266d",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 4 - Backward Elimination<a class=\"anchor\" id=\"DS106L4_page_4\"></a>\n",
    "\n",
    "[Back to Top](#DS106L4_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2ed6e-98dd-43f7-8761-6ecc9a13af08",
   "metadata": {},
   "source": [
    "# Backward Elimination\n",
    "\n",
    "Please watch **[this video](https://www.youtube.com/watch?v=0aTtMJO-pE4&t)** first.  What follows here won't make a lot of sense outside the context of the video.\n",
    "\n",
    "---\n",
    "\n",
    "## Load in Data\n",
    "\n",
    "First, load the built-in R data set called ```mtcars```. You will know if you have it if you type the command:\n",
    "\n",
    "```{r}\n",
    "head(mtcars)\n",
    "```\n",
    "\n",
    "And receive this in reply: \n",
    "\n",
    "```text\n",
    "                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\n",
    "Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n",
    "Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n",
    "Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n",
    "Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n",
    "Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n",
    "Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Question Setup\n",
    "\n",
    "Create a model that will use ```mpg``` as the response variable, and the other 10 columns of data as potential predictor variables. It is assumed that all 10 predictors don't really belong in the model. \n",
    "\n",
    "---\n",
    "\n",
    "## Get a Baseline\n",
    "\n",
    "You will start by creating a function called ```FitAll``` that creates a linear model of all 10 predictor variables. The command looks like this:\n",
    "\n",
    "```{r}\n",
    "FitAll = lm(mpg ~ ., data = mtcars)\n",
    "```\n",
    "\n",
    "```mpg``` is your y, or response variable, and ```.``` means \"all.\"  Instead of saying ```mpg ~ .```, you could have listed out all 10 predictor variables, as in \"mpg ~ cyl + disp + hp + drat + ...\". It is much easier to use the ```.``` rather than list them all out. \n",
    "\n",
    "Then get get the model summary, using the ```summary()``` function:\n",
    "\n",
    "```{r}\n",
    "summary(FitAll)\n",
    "```\n",
    "\n",
    "This provides you the following:\n",
    "\n",
    "```text\n",
    "\n",
    "Call:\n",
    "lm(formula = mpg ~ ., data = mtcars)\n",
    "\n",
    "Residuals:\n",
    "    Min      1Q  Median      3Q     Max \n",
    "-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n",
    "\n",
    "Coefficients:\n",
    "            Estimate Std. Error t value Pr(>|t|)  \n",
    "(Intercept) 12.30337   18.71788   0.657   0.5181  \n",
    "cyl         -0.11144    1.04502  -0.107   0.9161  \n",
    "disp         0.01334    0.01786   0.747   0.4635  \n",
    "hp          -0.02148    0.02177  -0.987   0.3350  \n",
    "drat         0.78711    1.63537   0.481   0.6353  \n",
    "wt          -3.71530    1.89441  -1.961   0.0633 .\n",
    "qsec         0.82104    0.73084   1.123   0.2739  \n",
    "vs           0.31776    2.10451   0.151   0.8814  \n",
    "am           2.52023    2.05665   1.225   0.2340  \n",
    "gear         0.65541    1.49326   0.439   0.6652  \n",
    "carb        -0.19942    0.82875  -0.241   0.8122  \n",
    "---\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Residual standard error: 2.65 on 21 degrees of freedom\n",
    "Multiple R-squared:  0.869,\tAdjusted R-squared:  0.8066 \n",
    "F-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n",
    "```\n",
    "\n",
    "Although the overall *p* value at the bottom is significant, none of the individual predictors are, meaning that the model is probably not a good fit; the number of variables has just inflated the *p* value to the point of being significant, and what the independent variables are does not really matter. \n",
    "\n",
    "Once you have created a new model after backward elimination, you can compare the above model summary with all 10 predictors against what R has helped determine is the best fit. \n",
    "\n",
    "---\n",
    "\n",
    "## Try Backward Elimination\n",
    "\n",
    "The real guts of the method is in the following line of code, using the ```step()``` function and specifying the argument ```direction='backward'``` to ensure you're doing backward elimination.\n",
    "\n",
    "```{r}\n",
    "step(FitAll, direction = 'backward')\n",
    "```\n",
    "\n",
    "What this bit of code does is creates a stepwise regression, going backward only on the linear model you just created using the ```mtcars``` dataset. \n",
    "\n",
    "Once you enter the command above, you get several pages of output. If you scroll up to the top of the output, right below the command, you will see this:\n",
    "\n",
    "```text\n",
    "Start:  AIC=70.9\n",
    "mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb\n",
    "```\n",
    "\n",
    "*AIC* stands for *Akaike Information Criteria*. It was invented by a Japanese statistician named Hirotugu Akaike in the 1970's. The AIC is a measure of relative quality of a model. However, it doesn't tell you if a model is any good or not. It simply helps you to compare two or more models against each other. When comparing two models, the model with the smaller AIC is generally accepted to be the better of the two models. In the case of the ```mtcars``` data, the start AIC of 70.9 is for the original model with all ten predictors.\n",
    "\n",
    "Now, look at the table below the start model:\n",
    "\n",
    "```text\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "- cyl   1    0.0799 147.57 68.915\n",
    "- vs    1    0.1601 147.66 68.932\n",
    "- carb  1    0.4067 147.90 68.986\n",
    "- gear  1    1.3531 148.85 69.190\n",
    "- drat  1    1.6270 149.12 69.249\n",
    "- disp  1    3.9167 151.41 69.736\n",
    "- hp    1    6.8399 154.33 70.348\n",
    "- qsec  1    8.8641 156.36 70.765\n",
    "<none>              147.49 70.898\n",
    "- am    1   10.5467 158.04 71.108\n",
    "- wt    1   27.0144 174.51 74.280\n",
    "```\n",
    "\n",
    "In this portion of the output, there are 11 rows of information. Each row is information pertaining to the single model shown above. The first column is unlabeled, but it contains the variable that has been removed for that particular row of output. For example, the first row says ```- cyl```. That means the first row is the model with only 9 predictor variables, where the ```cyl``` variable has been left out.\n",
    "\n",
    "Notice that for 10 potential predictor variables, there are 11 rows of output. This is a comparison of 11 models - the original model with all 10 predictors is included, as well as 10 different models where each model has had exactly one predictor variable removed. You will note that there is a model on the 9th row that is labeled as ```<none>```. This is the model with no predictors removed, or the original model. Note that the AIC in the far right column is 70.898 for that model, which is essentially the same as the ```Start``` model AIC listed above the table of 70.9 (rounding has taken place).\n",
    "\n",
    "The table has been sorted from smallest AIC at the top to largest AIC at the bottom. The takeaway from this table is as follows: Of all the models with just 9 predictors, it would appear that the model that excludes ```cyl``` is the best, because it has the smallest AIC.\n",
    "\n",
    "Before moving on, there are a couple important points to make:\n",
    "\n",
    "1. You don't really know if the model with ```cyl``` removed is any good or not, you just know that it is better than all the other models with a single predictor removed.\n",
    "\n",
    "2. You might assume at this point that all of the terms above the row with \"<none>\" could easily be removed. Since individually removing each of those rows makes for a better model, why not just remove all of them in one fell swoop, just to save time? This is actually an erroneous thought process. The power of stepwise regression is that each iteration evaluates a model in the presence or absence of the other terms in the model. This will be illustrated shortly based on the fact that at this first iteration, it might appear that the only terms that belong in the model are ```wt``` and ```am```.\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration Two\n",
    "\n",
    "Move onto the next iteration:\n",
    "\n",
    "```text\n",
    "Step:  AIC=68.92\n",
    "mpg ~ disp + hp + drat + wt + qsec + vs + am + gear + carb\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "- vs    1    0.2685 147.84 66.973\n",
    "- carb  1    0.5201 148.09 67.028\n",
    "- gear  1    1.8211 149.40 67.308\n",
    "- drat  1    1.9826 149.56 67.342\n",
    "- disp  1    3.9009 151.47 67.750\n",
    "- hp    1    7.3632 154.94 68.473\n",
    "<none>              147.57 68.915\n",
    "- qsec  1   10.0933 157.67 69.032\n",
    "- am    1   11.8359 159.41 69.384\n",
    "- wt    1   27.0280 174.60 72.297\n",
    "```\n",
    "\n",
    "This is the next portion of the output. Notice that there is a new start AIC of 68.92. This is because the predictor ```cyl``` has been removed from the model, based on the results above. In fact, right below the AIC is the current model, which has ```mpg``` as a function of ```disp```, ```hp```, ```drat```, ```wt```, ```qsec```, ```vs```, ```am```, ```gear```, and ```carb```. There is no ```cyl``` in the model.\n",
    "\n",
    "Below the new model, there is a table of 10 different models (9 predictor variables, removed one at a time, and the 10th model with no predictor variables removed). The output suggests that ```vs``` can be removed from the model to make it better. \n",
    "\n",
    "---\n",
    "\n",
    "### Iteration Three\n",
    "\n",
    "The next iteration will be calculated with both the ```cyl``` term and the ```vs``` terms removed:\n",
    "\n",
    "```text\n",
    "Step:  AIC=66.97\n",
    "mpg ~ disp + hp + drat + wt + qsec + am + gear + carb\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "- carb  1    0.6855 148.53 65.121\n",
    "- gear  1    2.1437 149.99 65.434\n",
    "- drat  1    2.2139 150.06 65.449\n",
    "- disp  1    3.6467 151.49 65.753\n",
    "- hp    1    7.1060 154.95 66.475\n",
    "<none>              147.84 66.973\n",
    "- am    1   11.5694 159.41 67.384\n",
    "- qsec  1   15.6830 163.53 68.200\n",
    "- wt    1   27.3799 175.22 70.410\n",
    "```\n",
    "\n",
    "The model with only 8 predictor terms has an AIC of 66.97. According to the table below the current model, if ```carb``` is removed, the AIC will go down a bit more. \n",
    "\n",
    "---\n",
    "\n",
    "### Iteration Four\n",
    "\n",
    "The model with only 7 predictors looks like this:\n",
    "\n",
    "```text\n",
    "Step:  AIC=65.12\n",
    "mpg ~ disp + hp + drat + wt + qsec + am + gear\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "- gear  1     1.565 150.09 63.457\n",
    "- drat  1     1.932 150.46 63.535\n",
    "<none>              148.53 65.121\n",
    "- disp  1    10.110 158.64 65.229\n",
    "- am    1    12.323 160.85 65.672\n",
    "- hp    1    14.826 163.35 66.166\n",
    "- qsec  1    26.408 174.94 68.358\n",
    "- wt    1    69.127 217.66 75.350\n",
    "```\n",
    "\n",
    "Now the AIC is at 65.12.\n",
    "\n",
    "Have you been paying attention to the models below the ```<none>``` row in each table? Remember, the interpretation of those rows is that if those variables get removed, the model will actually be a bit worse than the current model. Have you noticed that at each iteration, the variables below the ```<none>``` line are not always the same variables?\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration Five\n",
    "\n",
    "Move on to the next iteration:\n",
    "\n",
    "```text\n",
    "Step:  AIC=63.46\n",
    "mpg ~ disp + hp + drat + wt + qsec + am\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "- drat  1     3.345 153.44 62.162\n",
    "- disp  1     8.545 158.64 63.229\n",
    "<none>              150.09 63.457\n",
    "- hp    1    13.285 163.38 64.171\n",
    "- am    1    20.036 170.13 65.466\n",
    "- qsec  1    25.574 175.67 66.491\n",
    "- wt    1    67.572 217.66 73.351\n",
    "```\n",
    "\n",
    "You are down to six terms from ten. And it looks like you still have one or two terms you can eliminate...\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration Six\n",
    "\n",
    "Removing ```drat``` is next:\n",
    "\n",
    "```text\n",
    "Step:  AIC=62.16\n",
    "mpg ~ disp + hp + wt + qsec + am\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "- disp  1     6.629 160.07 61.515\n",
    "<none>              153.44 62.162\n",
    "- hp    1    12.572 166.01 62.682\n",
    "- qsec  1    26.470 179.91 65.255\n",
    "- am    1    32.198 185.63 66.258\n",
    "- wt    1    69.043 222.48 72.051\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration Seven\n",
    "\n",
    "Now removing ```disp``` is next. This might be the last term that is eliminated, looking at the output:\n",
    "\n",
    "```text\n",
    "Step:  AIC=61.52\n",
    "mpg ~ hp + wt + qsec + am\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "- hp    1     9.219 169.29 61.307\n",
    "<none>              160.07 61.515\n",
    "- qsec  1    20.225 180.29 63.323\n",
    "- am    1    25.993 186.06 64.331\n",
    "- wt    1    78.494 238.56 72.284\n",
    "```\n",
    "\n",
    "But it turns out that once ```disp``` was removed from the model, that all of a sudden it looks like ```hp``` doesn't belong in the model, either, whereas earlier it looks like it might. \n",
    "\n",
    "---\n",
    "\n",
    "### Iteration Eight\n",
    "\n",
    "So now remove ```hp``` from the model:\n",
    "\n",
    "```text\n",
    "Step:  AIC=61.31\n",
    "mpg ~ wt + qsec + am\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "<none>              169.29 61.307\n",
    "- am    1    26.178 195.46 63.908\n",
    "- qsec  1   109.034 278.32 75.217\n",
    "- wt    1   183.347 352.63 82.790\n",
    "```\n",
    "\n",
    "You only have 3 terms left in the model now. They are ```wt```, ```qsec```, and ```am```. If you look for a description of the mtcars dataset on the web, you will discover that ```wt``` is the weight of the car in thousands of pounds, ```qsec``` is the time it takes for the car to cover 1/4 mile from rest, and ```am``` is a coded variable for the type of transmission (0 for automatic, and 1 for manual).\n",
    "\n",
    "---\n",
    "\n",
    "## The Final Model\n",
    "\n",
    "According to the output table, the model that includes all three of those variables is better than any of the three models that omits one of those three variables. You are done with creating a model using backward elimination. If you haven't reached this conclusion on your own, don't worry, R has reached it for you! \n",
    "\n",
    "The next little bit of output tells not only which terms belong in the model, but what the coefficients are, too. You will see the following:\n",
    "\n",
    "```text\n",
    "Call:\n",
    "lm(formula = mpg ~ wt + qsec + am, data = mtcars)\n",
    "\n",
    "Coefficients:\n",
    "(Intercept)           wt         qsec           am  \n",
    "      9.618       -3.917        1.226        2.936  \n",
    "```\n",
    "\n",
    "This output tells you that the best model for the ```mtcars``` data uses ```wt```, ```qsec```, and ```am``` to predict the ```mpg``` for a car, using the following equation:\n",
    "\n",
    "```mpg = 9.618 + (-3.917) _ wt + 1.226 _ qsec + 2.936 \\* am```\n",
    "\n",
    "Pay attention to the signs of the coefficients for a moment:\n",
    "\n",
    "* The coefficient of ```wt``` is (-3.917). What that means is as long as everything else is equal, an increase of a car's weight by 1000 lbs will lead to an estimated decrease in the car's ```mpg``` by about 3.9 miles per gallon of gas.\n",
    "* The coefficient of ```qsec``` is 1.226. Everything else being equal, as a car's ```qsec``` increases, so does the ```mpg```. Does that make sense? Well, yes - if the ```qsec``` is larger, that means the car is slower off the start. If you know anything about cars, more power is usually associated with more speed, which is usually associated with worse gas mileage. In this case, you are being told that less power leads to higher gas mileage - for every additional second it takes to run a quarter mile, the gas mileage is improved by about 1.2 miles per gallon. It is pretty much an accepted fact that gutless cars get better gas mileage than fast cars.\n",
    "* The coefficient for the transmission type indicates that all other things being equal, two identical cars where one is an automatic transmission and one is a manual transmission will have different miles per gallon ratings. The manual transmission should get around 3 mpg better mileage than the automatic transmission.\n",
    "\n",
    "So now you have a model. Is it any good? You know it is the best model you could find using backward elimination, but is it any good?\n",
    "\n",
    "You can determine this by creating a new model with just those variables and then looking at the summary. You will create a new function called ```fitsome```, which will just have the terms you are interested in:\n",
    "\n",
    "```{r}\n",
    "fitsome = lm(mpg ~ am + qsec + wt, data = mtcars)\n",
    "```\n",
    "\n",
    "And here is the output:\n",
    "\n",
    "```text\n",
    "Call:\n",
    "lm(formula = mpg ~ am + qsec + wt, data = mtcars)\n",
    "\n",
    "Residuals:\n",
    "    Min      1Q  Median      3Q     Max \n",
    "-3.4811 -1.5555 -0.7257  1.4110  4.6610 \n",
    "\n",
    "Coefficients:\n",
    "            Estimate Std. Error t value Pr(>|t|)    \n",
    "(Intercept)   9.6178     6.9596   1.382 0.177915    \n",
    "am            2.9358     1.4109   2.081 0.046716 *  \n",
    "qsec          1.2259     0.2887   4.247 0.000216 ***\n",
    "wt           -3.9165     0.7112  -5.507 6.95e-06 ***\n",
    "---\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Residual standard error: 2.459 on 28 degrees of freedom\n",
    "Multiple R-squared:  0.8497,\tAdjusted R-squared:  0.8336 \n",
    "F-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11\n",
    "```\n",
    "\n",
    "There are a few things to note here from the above output:\n",
    "\n",
    "* There are coefficients for just the three predictor variables, as well as an intercept.\n",
    "* There is a multiple R<sup>2</sup> = 0.8497. The practical interpretation of this is that the model explains 84.97% of the variation in the ```mpg``` variable, and there is another 15.03% of the variation that can be chalked up to noise or random error.\n",
    "* There is an adjusted R<sup>2</sup> = 0.8336. The adjustment is a modification that is supposed to take into account the number of terms in the model. For your purpose, you are more interested in the adjusted R<sup>2</sup> than the multiple R<sup>2</sup>.\n",
    "* There is an *F*-statistic (52.75 with 3 and 28 degrees of freedom) and a *p* value of 0.0000000000121 (this is represented in scientific notation in R) which indicates that the model is better than no model at all, because the *p* value is less than 0.05.\n",
    "\n",
    "So, have you really gained anything by doing the backward elimination? To the untrained eye, one might say \"no gain.\" The R<sup>2</sup> values are very similar, and both models are statistically significant. However, to the trained eye, you have adhered to the \"Principle of Parsimony\" and come up with an equally good (maybe marginally better) model that is much simpler. Further, now all of your individual predictors are significant! Mission accomplished.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4ad3c-2fe6-457e-8924-1410e951bebd",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 5 - Forward Selection<a class=\"anchor\" id=\"DS106L4_page_5\"></a>\n",
    "\n",
    "[Back to Top](#DS106L4_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc06a2-e702-47a1-942f-cbcbe31221a7",
   "metadata": {},
   "source": [
    "# Forward Selection\n",
    "\n",
    "Please watch **[this video](https://www.youtube.com/watch?v=OYEII--K_k4&t)** on forward selection.\n",
    "\n",
    "The difference between backward elimination and forward selection is hopefully pretty obvious by now. Backward elimination starts with all the predictors, and then removes them one by one until you are left with a model that is better than any model with one more predictor removed. On the other hand, forward selection starts with no predictors, and adds predictors one at a time until adding one more predictor does not appreciably improve the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Create the Original Model\n",
    "\n",
    "The commands are similar to those of backward elimination, but instead of starting with all the predictors, you start with none of the predictors.  This done by subbing out the ```.``` in the model for a ```1```: \n",
    "\n",
    "```{r}\n",
    "fitstart = lm(mpg ~ 1, data = mtcars)\n",
    "```\n",
    "\n",
    "In this function, the part where it says ```mpg ~ 1```, tells the model to start without any predictors; the ```1``` part forces the model to simply use the average ```mpg``` for the dataset as the predicted value of ```mpg```. It is often called the *naive model*, meaning that it is a model where you assume all future ```mpg``` fuel ratings are simply equal to the mean of the historical fuel ratings.\n",
    "\n",
    "Then run a summary for this model: \n",
    "\n",
    "```{r}\n",
    "summary(fitstart)\n",
    "```\n",
    "\n",
    "You will get the following:\n",
    "\n",
    "```text\n",
    "Call:\n",
    "lm(formula = mpg ~ 1, data = mtcars)\n",
    "\n",
    "Residuals:\n",
    "    Min      1Q  Median      3Q     Max \n",
    "-9.6906 -4.6656 -0.8906  2.7094 13.8094 \n",
    "\n",
    "Coefficients:\n",
    "            Estimate Std. Error t value Pr(>|t|)    \n",
    "(Intercept)   20.091      1.065   18.86   <2e-16 ***\n",
    "---\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Residual standard error: 6.027 on 31 degrees of freedom\n",
    "```\n",
    "\n",
    "Notice that the estimate for the intercept is 20.091, which is simply the average mpg for the 28 vehicles in the original ```mtcars``` dataset. This is a model, but it is a poor one, even though it is significant.  \n",
    "\n",
    "---\n",
    "\n",
    "## Begin Forward Selection\n",
    "\n",
    "At this point, you will use the ```step()``` function to complete forward selection. R needs some instruction about which variables are potential predictors. This is called the scope of the stepwise approach. You could either spell out the entire scope of the potential model as follows:\n",
    "\n",
    "```{r}\n",
    "step(fitstart, direction = 'forward', scope = (~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb))\n",
    "```\n",
    "\n",
    "Or you can utilize the fact that the ```FitAll``` model was defined before when you did the backward elimination approach, and simplify the command as follows:\n",
    "\n",
    "```{r}\n",
    "step(fitstart, direction = 'forward', scope = (formula(FitAll)))\n",
    "```\n",
    "\n",
    "In either case, ```direction = 'forward'``` specifies that you are doing forward selection, and the argument ```scope=``` specifies the model that ```step()``` is moving towards.\n",
    "\n",
    "Here is what the results look like, one step at a time.\n",
    "\n",
    "---\n",
    "\n",
    "### First Iteration\n",
    "\n",
    "The output below shows that the AIC for the model with no terms at all is 115.94. If you look at the table below the AIC, you can see that adding _any_ predictor variable will create a better model than the model with no predictors, because any model with one variable has an AIC smaller than 115.94. Since the predictors are sorted, from best to worst, it appears that ```wt``` is the best individual predictor, so it should be added to the model first.\n",
    "\n",
    "```text\n",
    "Start:  AIC=115.94\n",
    "mpg ~ 1\n",
    "\n",
    "       Df Sum of Sq     RSS     AIC\n",
    "+ wt    1    847.73  278.32  73.217\n",
    "+ cyl   1    817.71  308.33  76.494\n",
    "+ disp  1    808.89  317.16  77.397\n",
    "+ hp    1    678.37  447.67  88.427\n",
    "+ drat  1    522.48  603.57  97.988\n",
    "+ vs    1    496.53  629.52  99.335\n",
    "+ am    1    405.15  720.90 103.672\n",
    "+ carb  1    341.78  784.27 106.369\n",
    "+ gear  1    259.75  866.30 109.552\n",
    "+ qsec  1    197.39  928.66 111.776\n",
    "<none>              1126.05 115.943\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Second Iteration\n",
    "\n",
    "Now, ```wt``` is in the model: \n",
    "\n",
    "```text\n",
    "Step:  AIC=73.22\n",
    "mpg ~ wt\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "+ cyl   1    87.150 191.17 63.198\n",
    "+ hp    1    83.274 195.05 63.840\n",
    "+ qsec  1    82.858 195.46 63.908\n",
    "+ vs    1    54.228 224.09 68.283\n",
    "+ carb  1    44.602 233.72 69.628\n",
    "+ disp  1    31.639 246.68 71.356\n",
    "<none>              278.32 73.217\n",
    "+ drat  1     9.081 269.24 74.156\n",
    "+ gear  1     1.137 277.19 75.086\n",
    "+ am    1     0.002 278.32 75.217\n",
    "```\n",
    "\n",
    "You can next see that the next term to add is ```cyl```. As a reminder, the new AIC on the row labeled ```cyl``` is the quality of the model with ```cyl``` added, and in the presence of ```wt``` already in the model, and not the quality of having ```cyl``` in the model by itself. If you compare this table with the table above, it is clear that having both ```wt``` and ```cyl``` in the model is better than just having ```wt``` or just having ```cyl```.\n",
    "\n",
    "---\n",
    "\n",
    "### Third Iteration\n",
    "\n",
    "With ```wt``` and ```cyl``` already in the model, it still makes sense to add ```hp``` to get some improvement.\n",
    "\n",
    "```text\n",
    "Step:  AIC=63.2\n",
    "mpg ~ wt + cyl\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "+ hp    1   14.5514 176.62 62.665\n",
    "+ carb  1   13.7724 177.40 62.805\n",
    "<none>              191.17 63.198\n",
    "+ qsec  1   10.5674 180.60 63.378\n",
    "+ gear  1    3.0281 188.14 64.687\n",
    "+ disp  1    2.6796 188.49 64.746\n",
    "+ vs    1    0.7059 190.47 65.080\n",
    "+ am    1    0.1249 191.05 65.177\n",
    "+ drat  1    0.0010 191.17 65.198\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Fourth Iteration\n",
    "\n",
    "Now, with a model including ```wt```, ```cyl```, and ```hp```, you are done. There is no point in adding any more terms, because according to the table, the AIC will actually increase if you do.\n",
    "\n",
    "```text\n",
    "Step:  AIC=62.66\n",
    "mpg ~ wt + cyl + hp\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "<none>              176.62 62.665\n",
    "+ am    1    6.6228 170.00 63.442\n",
    "+ disp  1    6.1762 170.44 63.526\n",
    "+ carb  1    2.5187 174.10 64.205\n",
    "+ drat  1    2.2453 174.38 64.255\n",
    "+ qsec  1    1.4010 175.22 64.410\n",
    "+ gear  1    0.8558 175.76 64.509\n",
    "+ vs    1    0.0599 176.56 64.654\n",
    "\n",
    "Call:\n",
    "lm(formula = mpg ~ wt + cyl + hp, data = mtcars)\n",
    "\n",
    "Coefficients:\n",
    "(Intercept)           wt          cyl           hp  \n",
    "   38.75179     -3.16697     -0.94162     -0.01804  \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Examine the Final Model\n",
    "\n",
    "Do the same thing you did for the backward elimination model: build a model that only contains ```wt```, ```cyl```, and ```hp```. You called it ```fitsome``` in the backward elimination model; call this one ```fitsome2```:\n",
    "\n",
    "```{r}\n",
    "fitsome2 = lm(mpg ~ wt + cyl + hp, data = mtcars)\n",
    "```\n",
    "\n",
    "And then look at the summary:\n",
    "\n",
    "```{r}\n",
    "summary(fitsome2)\n",
    "```\n",
    "\n",
    "Here are the results:\n",
    "\n",
    "```text\n",
    "Call:\n",
    "lm(formula = mpg ~ wt + cyl + hp, data = mtcars)\n",
    "\n",
    "Residuals:\n",
    "    Min      1Q  Median      3Q     Max \n",
    "-3.9290 -1.5598 -0.5311  1.1850  5.8986 \n",
    "\n",
    "Coefficients:\n",
    "            Estimate Std. Error t value Pr(>|t|)    \n",
    "(Intercept) 38.75179    1.78686  21.687  < 2e-16 ***\n",
    "wt          -3.16697    0.74058  -4.276 0.000199 ***\n",
    "cyl         -0.94162    0.55092  -1.709 0.098480 .  \n",
    "hp          -0.01804    0.01188  -1.519 0.140015    \n",
    "---\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Residual standard error: 2.512 on 28 degrees of freedom\n",
    "Multiple R-squared:  0.8431,\tAdjusted R-squared:  0.8263 \n",
    "F-statistic: 50.17 on 3 and 28 DF,  p-value: 2.184e-11\n",
    "```\n",
    "\n",
    "When you did the backward elimination model, you compared the initial model (all ten predictors) with the optimized model, and concluded that the models are similar as far as R<sup>2</sup> is concerned, so the model with just 3 predictors seems preferable based upon simplicity alone. For the forward selection model, there really is no starting point sort of model. You just used the average mpg to get a starting point without any predictor variables.\n",
    "\n",
    "Nonetheless, take a look at the specifics of the forward selection model shown above:\n",
    "\n",
    "* The multiple R<sup>2</sup> = 0.8431. The practical interpretation of this is that the model explains 84.31% of the variation in the ```mpg``` variable, and there is another 15.69% of the variation that can be chalked up to noise or random error.\n",
    "* There is an adjusted R<sup>2</sup> = 0.8263. The \"adjustment\" is a modification that is supposed to take into account the number of terms in the model. For your purposes, you are more interested in the adjusted R<sup>2</sup> than the multiple R<sup>2</sup>.\n",
    "* There is an *F*-statistic (50.17 with 3 and 28 degrees of freedom) and a *p* value of 0.00000000002184 (this is represented in scientific notation in R) which indicates that the model is better than no model at all, because the *p* value is less than 0.05.\n",
    "\n",
    "The R<sup>2</sup> for the backward elimination model and the forward selection model are similar. So, which one is better? One could argue that the AIC for the backward elimination model is slightly lower, so it must be better. But for all practical purposes, they are essentially the same. However, did you notice the terms in the predictor model are not the same? That is actually a pretty common result. The truth is that in many cases, there might not just be a single 'best' model that is far superior to the 'second best' model, but there might be a cluster of models that are essentially equally good.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e3b8a-9224-447f-8cf7-5035cae86421",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 6 - Hybrid Stepwise - Forward and Backward Selection<a class=\"anchor\" id=\"DS106L4_page_6\"></a>\n",
    "\n",
    "[Back to Top](#DS106L4_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e2098-28ef-4ff5-a155-afe824de4323",
   "metadata": {},
   "source": [
    "# Hybrid Stepwise - Forward and Backward Selection\n",
    "\n",
    "**[This video](https://www.youtube.com/watch?v=ejR8LnQziPY)** shows how to do the regression method that does both forward steps and backward steps. In general terms, you start with no predictors, and use the mean value for ```mpg``` only. This is the same way you started the forward selection model. You will use the ```step()``` function again, but this time, utilize ```direction=\"both\"``` as an argument, to specify the hybrid stepwise approach: \n",
    "\n",
    "```{r}\n",
    "step(fitstart, direction=\"both\", scope=formula(FitAll))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## First Iteration \n",
    "\n",
    "\n",
    "Here is what the first iteration looks like.  You will notice that it is identical to the first iteration of the 'forward selection approach.' Adding ```wt``` to the model is the first step.\n",
    "\n",
    "```text\n",
    "Start:  AIC=115.94\n",
    "mpg ~ 1\n",
    "\n",
    "       Df Sum of Sq     RSS     AIC\n",
    "+ wt    1    847.73  278.32  73.217\n",
    "+ cyl   1    817.71  308.33  76.494\n",
    "+ disp  1    808.89  317.16  77.397\n",
    "+ hp    1    678.37  447.67  88.427\n",
    "+ drat  1    522.48  603.57  97.988\n",
    "+ vs    1    496.53  629.52  99.335\n",
    "+ am    1    405.15  720.90 103.672\n",
    "+ carb  1    341.78  784.27 106.369\n",
    "+ gear  1    259.75  866.30 109.552\n",
    "+ qsec  1    197.39  928.66 111.776\n",
    "<none>              1126.05 115.943\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Second Iteration\n",
    "\n",
    "Notice at the bottom of this table shown above. It not only specifies what the new AIC would be if a predictor were aded (as indicated by the \"+\" sign at the start of each row), but also specifies what the new AIC would be if ```wt``` were removed from the model (as indicated by the \"-\" at the start of the row with ```wt```). It would be silly to add ```wt``` in one step, and then turn around and immediately remove it in the next step. However, when there are more than two terms in the model, this isn't such a far fetched idea.\n",
    "\n",
    "```text\n",
    "Step:  AIC=73.22\n",
    "mpg ~ wt\n",
    "\n",
    "       Df Sum of Sq     RSS     AIC\n",
    "+ cyl   1     87.15  191.17  63.198\n",
    "+ hp    1     83.27  195.05  63.840\n",
    "+ qsec  1     82.86  195.46  63.908\n",
    "+ vs    1     54.23  224.09  68.283\n",
    "+ carb  1     44.60  233.72  69.628\n",
    "+ disp  1     31.64  246.68  71.356\n",
    "<none>               278.32  73.217\n",
    "+ drat  1      9.08  269.24  74.156\n",
    "+ gear  1      1.14  277.19  75.086\n",
    "+ am    1      0.00  278.32  75.217\n",
    "- wt    1    847.73 1126.05 115.943\n",
    "```\n",
    "\n",
    "The second iteration suggests that the best way to improve the model is to add the ```cyl``` predictor, which is what you will do in the next iteration below!\n",
    "\n",
    "---\n",
    "\n",
    "## Third Iteration\n",
    "\n",
    "Now, with ```cyl``` in the model also, each term that is not already in the model becomes a candidate for inclusion, and each term already in the model becomes a candidate for elimination. As usual, you will let the AIC dictate what happens. In this case, the model is improved most by adding the ```hp``` term. \n",
    "\n",
    "```text\n",
    "Step:  AIC=63.2\n",
    "mpg ~ wt + cyl\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "+ hp    1    14.551 176.62 62.665\n",
    "+ carb  1    13.772 177.40 62.805\n",
    "<none>              191.17 63.198\n",
    "+ qsec  1    10.567 180.60 63.378\n",
    "+ gear  1     3.028 188.14 64.687\n",
    "+ disp  1     2.680 188.49 64.746\n",
    "+ vs    1     0.706 190.47 65.080\n",
    "+ am    1     0.125 191.05 65.177\n",
    "+ drat  1     0.001 191.17 65.198\n",
    "- cyl   1    87.150 278.32 73.217\n",
    "- wt    1   117.162 308.33 76.494\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Fourth Iteration\n",
    "\n",
    "With the fourth iteration, the model suggests that having ```wt```, ```cyl```, and ```hp``` as predictors is better than any other model with another single predictor added, and better than any other model with one of those predictors removed. The iterative process has stabilized, and you have the same model as you did when you did the forward selection approach.\n",
    "\n",
    "```text\n",
    "Step:  AIC=62.66\n",
    "mpg ~ wt + cyl + hp\n",
    "\n",
    "       Df Sum of Sq    RSS    AIC\n",
    "<none>              176.62 62.665\n",
    "- hp    1    14.551 191.17 63.198\n",
    "+ am    1     6.623 170.00 63.442\n",
    "+ disp  1     6.176 170.44 63.526\n",
    "- cyl   1    18.427 195.05 63.840\n",
    "+ carb  1     2.519 174.10 64.205\n",
    "+ drat  1     2.245 174.38 64.255\n",
    "+ qsec  1     1.401 175.22 64.410\n",
    "+ gear  1     0.856 175.76 64.509\n",
    "+ vs    1     0.060 176.56 64.654\n",
    "- wt    1   115.354 291.98 76.750\n",
    "```\n",
    "\n",
    "You are again at mission accomplished. You won't bother to run a new model with those three predictors, since you already did that above.\n",
    "\n",
    "---\n",
    "\n",
    "## Hierarchical Regression \n",
    "\n",
    "In *hierarchical regression*, you get to pick the variables that are being added or removed next, which allows you to make statements about how much variance is added \"over and above\" other variables. This approach can be quite useful especially when answering stakeholder questions about what variables are most important. \n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Stepwise regression has three basic approaches - backward elimination, forward selection, and a combination of the two.\n",
    "* Backward elimination creates a model with all predictor variables included, and then removes them one at a time until the model is optimized.\n",
    "* Forward selection starts with no predictor variables included, and adds them one at a time until the model is optimized.\n",
    "* The combination approach starts with no predictor terms. Every time a term is either added or removed from the model, it is compared with the quality of all other models that either add a single predictor or remove a single predictor until the model is optimized.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb7d8a-aced-42e5-ac24-32acf8c95473",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 7 - Key Terms<a class=\"anchor\" id=\"DS106L4_page_7\"></a>\n",
    "\n",
    "[Back to Top](#DS106L4_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0debaf-5fae-4035-8e7d-ae62e62456cb",
   "metadata": {},
   "source": [
    "# Key Terms\n",
    "\n",
    "Below is a list and short description of the important keywords learned in this lesson. Please read through and go back and review any concepts you do not fully understand. Great Work!\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Stepwise Regression</td>\n",
    "        <td>A process to determine exactly what variables make up the best-fitting regression model.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Multiple Regression</td>\n",
    "        <td>A regression with multiple independent variables.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Overfitting</td>\n",
    "        <td>When a model fits the data so well that it won't fit future data.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Principle of Parsimony</td>\n",
    "        <td>When all other things are equal, go with the simplest explanation.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Backward Elimination</td>\n",
    "        <td>Starting with all the predictors and removing them one at a time to find the best-fitting model.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Forward Selection</td>\n",
    "        <td>Starting with no predictors and adding them one at a time to find the best-fitting model.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Hybrid Stepwise Regression</td>\n",
    "        <td>Starting with no predictors and adding or removing them one at a time to find the best-fitting model.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Akaike Information Criteria (AIC)</td>\n",
    "        <td>An indicator of model fit quality. The smaller, the better.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## Key R Code\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>step()</td>\n",
    "        <td>A function to perform stepwise regression.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>direction=\"backward\"</td>\n",
    "        <td>An argument to step() that performs backward elimination.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>direction=\"forward\"</td>\n",
    "        <td>An argument to step() that performs forward selection.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>scope=</td>\n",
    "        <td>An argument to step() that specifies what model you are moving towards.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>direction=\"both\"</td>\n",
    "        <td>An argument to step() that specifies hybrid stepwise regression.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3f800-0a14-4d13-8c32-390965c537ca",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 8 - Lesson 4 Practice Hands-On<a class=\"anchor\" id=\"DS106L4_page_8\"></a>\n",
    "\n",
    "[Back to Top](#DS106L4_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597e4b4-ce22-448d-843e-eab5ba74a9c4",
   "metadata": {},
   "source": [
    "# Stepwise Regression Hands On\n",
    "\n",
    "This Hands-On will **not** be graded, but you are encouraged to complete it. The best way to become a great data scientist is to practice!\n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Do not submit your project until you have completed all requirements, as you will not be able to resubmit.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Part I: Backwards Elimination\n",
    "\n",
    "Use **[this data file](https://repo.exeterlms.com/documents/V2/DataScience/Modeling-Optimization/IQ.zip)**, which contains test scores and IQ for 15 individuals. Each individual took 5 tests. The IQ is the response variable, and the five different tests are the potential predictor variables. Perform a backwards elimination on this data, then answer the following questions:\n",
    "\n",
    "* Which model is the best? Why?\n",
    "* From the best model, what is the adjusted R<sup>2</sup> value and what does it mean? \n",
    "* From the best model, how does each variable influence IQ?\n",
    "\n",
    "---\n",
    "\n",
    "## Part II: Compare Stepwise Regression Types\n",
    "\n",
    "The **[following dataset](https://repo.exeterlms.com/documents/V2/DataScience/Modeling-Optimization/stepwiseRegression.zip)** will be used for this analysis. This data has a single response (Y) variable, and twelve predictor (X1 through X12) variables. Use these data to run all three kinds of stepwise regressions (backward elimination, forward selection, and the hybrid method). After completing these analyses, answer the following questions:\n",
    "\n",
    "* Which model was the best for each type of method?\n",
    "* How do the final models from each method compare to each other?\n",
    "* From your chosen \"best model,\" explain what variable(s) contribute to predicting Y and for how much variance they account.\n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Be sure to zip and submit your entire directory when finished!</p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36943bcd-7e16-4af1-97d3-c09e81ae5e99",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 9 - Lesson 4 Practice Hands-On Solution<a class=\"anchor\" id=\"DS106L4_page_9\"></a>\n",
    "\n",
    "[Back to Top](#DS106L4_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2cdde-9447-49b5-8936-17d80f46b45e",
   "metadata": {},
   "source": [
    "# Lesson 4 Practice Hands-On Solution\n",
    "\n",
    "Below you will find the solution to the Lesson 4 hands-on!\n",
    "\n",
    "---\n",
    "\n",
    "## Part I\n",
    "\n",
    "```r\n",
    "#Backward Selection\n",
    "FitAll1 <- lm(IQ ~ ., data = IQ)\n",
    "\n",
    "summary(FitAll1)\n",
    "\n",
    "step(FitAll1, direction = 'backward')\n",
    "\n",
    "#Create another model for only selected variables and compare their results\n",
    "fitsome <- lm(IQ ~ Test1 + Test2 + Test4, data = IQ)\n",
    "summary(fitsome)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part II\n",
    "\n",
    "```r\n",
    "#Backward Selection\n",
    "FitAll = lm(Y ~ ., data = stepwiseRegression)\n",
    "summary(FitAll)\n",
    "\n",
    "step(FitAll, direction = 'backward', scope = formula(FitAll))\n",
    "\n",
    "#Forward Selection\n",
    "fitstart = lm(Y ~ 1, data = stepwiseRegression)\n",
    "summary(fitstart)\n",
    "\n",
    "step(fitstart, direction = 'forward', scope = (formula(FitAll)))\n",
    "\n",
    "#StepWise \n",
    "step(fitstart, direction = 'both', scope = formula(FitAll))\n",
    "\n",
    "fitsome <- lm(formula = Y ~ X2 + X4 + X6 + X10 + X11 + X12, data = stepwiseRegression)\n",
    "summary(fitsome)\n",
    "\n",
    "#Compare the output from the different types of stepwise selections above and note the order of the elimination of features\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
