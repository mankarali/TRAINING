{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e6ddbb-52d3-4a02-9b69-100a4181e3b0",
   "metadata": {},
   "source": [
    "# DS106 Machine Learning : Lesson Ten Companion Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd4bfc-9b15-4b80-87b4-b8b60eb56371",
   "metadata": {},
   "source": [
    "### Table of Contents <a class=\"anchor\" id=\"DS106L10_toc\"></a>\n",
    "\n",
    "* [Table of Contents](#DS106L10_toc)\n",
    "    * [Page 1 - Introduction](#DS106L10_page_1)\n",
    "    * [Page 2 - Natural Language Processing ](#DS106L10_page_2)\n",
    "    * [Page 3 - Read in Text](#DS106L10_page_3)\n",
    "    * [Page 4 - Convert Text to Soup](#DS106L10_page_4)\n",
    "    * [Page 5 - Tokenize Data](#DS106L10_page_5)\n",
    "    * [Page 6 - Remove Capitalization](#DS106L10_page_6)\n",
    "    * [Page 7 - Remove Stopwords](#DS106L10_page_7)\n",
    "    * [Page 8 - Count and Plot Words](#DS106L10_page_8)\n",
    "    * [Page 9 - Key Terms](#DS106L10_page_9)\n",
    "    * [Page 10 - ](#DS106L10_page_10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a8c65-ed48-4354-a257-e01b0fd733d4",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 1 - Overview of this Module<a class=\"anchor\" id=\"DS106L10_page_1\"></a>\n",
    "\n",
    "[Back to Top](#DS106L10_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7adea17-f27b-4043-b03f-352a715c7668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"480\"\n",
       "            src=\"https://player.vimeo.com/video/388630868\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x236c8d746d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "# Tutorial Video Name: Natural Language Processing\n",
    "VimeoVideo('388630868', width=720, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ad526-95fa-43c4-bc43-1fe807708ed3",
   "metadata": {},
   "source": [
    "\n",
    "The transcript for the above overview video **[is located here](https://repo.exeterlms.com/documents/V2/DataScience/Video-Transcripts/DSO106-ML-L05overview.zip)**.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "So much information is presented only as the written or spoken word, yet most of the tools you have learned so far for dealing with data won't handle text! In this lesson, you'll learn a technique called *Natural Language Processing* to handle raw text data and turn it into something usable.  By the end of this lesson, you should be able to:\n",
    "\n",
    "* Read in data from webpages\n",
    "* Convert text to a usable format\n",
    "* Utilize HTML tags to pull data parts\n",
    "* Tokenize your data\n",
    "* Use for loops to remove capitalization and stopwords\n",
    "* Count words used in novels and chart the most frequent ones\n",
    "\n",
    "This lesson will culminate in a hands-ons in which you find the most frequently occurring words in Lewis Carroll's *Alice's Adventures in Wonderland*.  Ready to start down the rabbit hole?\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Additional Info!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>You may want to watch this <a href=\"https://vimeo.com/441207100\"> recorded live workshop </a> that goes over the material in this lesson. </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990ac7a-5a2f-4744-af14-1b7e385ea484",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 2 - Natural Language Processing<a class=\"anchor\" id=\"DS106L10_page_2\"></a>\n",
    "\n",
    "[Back to Top](#DS106L10_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612429df-e4ef-4d85-8460-5bc8914079c3",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "While some information is nicely and neatly laid out in columns and rows, the majority of information is hidden in language! This would not be a problem if language were not such a confusing thing that no two people use in exactly the same way.  \n",
    "\n",
    "Imagine you are trying to process movie reviews to find out if something were popular.  The word \"bad\" might lead you to believe a review was negative, but in the context of \"this movie was bad arse!\" it might be a positive review.\n",
    "\n",
    "*Natural Language Processing (NLP)* is the process by which data scientists try to glean useful information out of the chaos that is the written word.\n",
    "\n",
    "## NLP in Python\n",
    "\n",
    "Now that you have a general idea of what natural language processing is, you'll learn how to do it in Python.  \n",
    "\n",
    "---\n",
    "\n",
    "## Install Required Packages\n",
    "\n",
    "You may need to install some packages before you begin. You'll need to open up your terminal and navigate to the folder where you are going to store your data for this exercise.  Then, run this code: \n",
    "\n",
    "```bash\n",
    "pip install bs4\n",
    "```\n",
    "\n",
    "Then, open up your Anaconda Command Prompt, and you will run the following: \n",
    "\n",
    "```bash\n",
    "conda install nltk\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Import those Packages\n",
    "\n",
    "\n",
    "Now that you've installed a few packages, you will need to import them, along with a lot of other stuff! \n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "```\n",
    "\n",
    "You'll use `requests` to read in your data from the webpage, `BeautifulSoup` to help process your raw data, `nltk` as the definitive natural language processing package, and `RegexpTokenizer` to break down your data into words. You should already be familiar with `matplotlib` and `seaborn`, and you'll make use of them to visualize the frequency counts of words at the end.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832358fd-55fa-420a-8aa2-46fbf2097986",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 3 - Read in Text<a class=\"anchor\" id=\"DS106L10_page_3\"></a>\n",
    "\n",
    "[Back to Top](#DS106L10_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564e3e7-3a7f-4936-b3a3-2e0a9ab78512",
   "metadata": {},
   "source": [
    "# Read in Text\n",
    "\n",
    "Then, you will read in the text you'll analyze. You'll start by taking a URL to a webpage and assigning it a variable name, in this case, `url`:\n",
    "\n",
    "```python\n",
    "url = 'http://www.gutenberg.org/files/1184/1184-h/1184-h.htm'\n",
    "```\n",
    "\n",
    "This particular URL above goes to an e-book, *The Count of Monte Cristo*, by Alexander Dumas.  If you haven't read it, you should - great read! Maybe when you're done learning all of data science and have room in your brain for fun again.  Once you have your URL saved to a Python variable, you can get make a request to get data from that webpage.  This will use the function `requests.get()`, and you are making it to the `url` webpage you just saved:\n",
    "\n",
    "```python\n",
    "r = requests.get(url)\n",
    "```\n",
    "\n",
    "Then, you can find out the type of that request if you like by using the function `type()`:\n",
    "\n",
    "```python\n",
    "type(r)\n",
    "```\n",
    "\n",
    "The response back you should receive is: \n",
    "\n",
    "```text\n",
    "requests.models.Response\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aade2-59af-4cbc-b5ed-da31e39f266d",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 4 - Convert Text to Soup<a class=\"anchor\" id=\"DS106L10_page_4\"></a>\n",
    "\n",
    "[Back to Top](#DS106L10_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2ed6e-98dd-43f7-8761-6ecc9a13af08",
   "metadata": {},
   "source": [
    "# Convert Text to Soup\n",
    "\n",
    "Yes, you read that right! Your next step is to make some soup. Chicken noodle is the recommendation, but feel free to go wild. Chili? Lemon orzo? A good beef stew? The possibilities are endless. The next few lines take the data off the webpage and extract the text, then use the `html5lib` to convert it into something you'll be able to process and better understand, called *soup*.\n",
    "\n",
    "```python\n",
    "html = r.text\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "type(soup)\n",
    "```\n",
    "\n",
    "You know the first two lines worked, because when you determine the type with that `type()` function, it will give you back:\n",
    "\n",
    "```text\n",
    "bs4.BeautifulSoup\n",
    "```\n",
    "\n",
    "Feel free to burst into song here. Need some Animal Crackers in your Soup? Bet that gets stuck in your head all day. You're welcome!\n",
    "\n",
    "---\n",
    "\n",
    "## Use HTML Tags to Extract Useful Info\n",
    "\n",
    "If you know your HTML, and the website is designed well, you can then call out certain pieces of this text.  For instance, the title:\n",
    "\n",
    "```python\n",
    "soup.title.string\n",
    "```\n",
    "\n",
    "`soup` is the name of the webpage broken into HTML, then you are calling the `title` tag from the HTML and asking for Python to give it back as `.string`.  The result should be this:\n",
    "\n",
    "```text\n",
    "'The Count of Monte Cristo, by Alexandre Dumas, pÃ¨re'\n",
    "```\n",
    "\n",
    "Which happens to be both the title of the book and the title of the webpage.\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Fun Fact!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>There are many other HTML tags, and you make use of them to get all sorts of information out of a webpage.  The problem is, each website is typically set up a little differently, and you'll actually need to peek at the structure of the webpage to make the right HTML tag calls. If you happen to already know HTML, this will be a great place to play around.  If not, don't worry - there are other ways to play with soup that aren't so messy.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4ad3c-2fe6-457e-8924-1410e951bebd",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 5 - Tokenize Data<a class=\"anchor\" id=\"DS106L10_page_5\"></a>\n",
    "\n",
    "[Back to Top](#DS106L10_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc06a2-e702-47a1-942f-cbcbe31221a7",
   "metadata": {},
   "source": [
    "# Tokenize Data\n",
    "\n",
    "No, all of you Lord of the Rings fans, stand down! J.R.R. Tolkein will not be making an appearance here.  Instead, you're learning about *tokens*, or your text, broken down into words. The following code uses the function `get_text()` to retrieve your text, and then you will use the function `RegexpTokenizer()` to break it down into words, separated by spaces.  Spaces, you might ask? Well, in the language RegEx, `\\w+` is the symbol for space.\n",
    "\n",
    "Then the function `tokenize()` will actually perform the operation, and you will get the first five words with `[:5]`:\n",
    "\n",
    "```python\n",
    "text = soup.get_text()\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens[:5]\n",
    "```\n",
    "\n",
    "Here is the result from this code:\n",
    "\n",
    "```text\n",
    "['The', 'Count', 'of', 'Monte', 'Cristo']\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e3b8a-9224-447f-8cf7-5035cae86421",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 6 - Remove Capitalization<a class=\"anchor\" id=\"DS106L10_page_6\"></a>\n",
    "\n",
    "[Back to Top](#DS106L10_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e2098-28ef-4ff5-a155-afe824de4323",
   "metadata": {},
   "source": [
    "# Remove Capitalization\n",
    "\n",
    "Notice that the above has capitalization.  That's great for when you are reading, but when you're trying to break things down for analysis, it is not as useful.  Should `The` be different from `the`? What about typos, like `THe` or `thE`? Probably not important.  You can remove capitalization like this:\n",
    "\n",
    "```python\n",
    "words = []\n",
    "for word in tokens:\n",
    "    words.append(word.lower())\n",
    "```\n",
    "\n",
    "The above code uses a for loop, and makes use of the function `lower()` to strip caps.  It all goes into a dictionary named `words`. You can take a look at the first five entries in the dictionary like this:\n",
    "\n",
    "```python\n",
    "words[:5]\n",
    "```\n",
    "\n",
    "And the result should be:\n",
    "\n",
    "```text\n",
    "['the', 'count', 'of', 'monte', 'cristo']\n",
    "```\n",
    "\n",
    "There.  You're now capitalization blind.\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Additional Info!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Removing capitalization is part of text pre-processing, and if you'd like to learn about other text pre-processing, <a href=\"https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html\"> check this article.com</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb7d8a-aced-42e5-ac24-32acf8c95473",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 7 - Remove Stopwords<a class=\"anchor\" id=\"DS106L10_page_7\"></a>\n",
    "\n",
    "[Back to Top](#DS106L10_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0debaf-5fae-4035-8e7d-ae62e62456cb",
   "metadata": {},
   "source": [
    "# Remove Stopwords\n",
    "\n",
    "There are some words that mean nothing out of context.  Writing a paper on how many times a book uses the word \"the\" is not very interesting.  Your future supervisor would not be impressed, unless you're going to go into the field of linguistics. These \"boring\" and \"useless\" words are considered *stopwords*, and, luckily, `nltk` already has a list of them! You can pull them out and label them like this:\n",
    "\n",
    "```python\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "```\n",
    "\n",
    "<div class=\"panel panel-info\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Tip!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>If you get an error that says something like \"no stopwords,\" then you'll need to download them.  You can go into your Anaconda Command Prompt (if you are using Jupyter) and get the stopword list by using this line: python -m nltk.downloader stopwords . Then re-run the line above and you should be good to go!</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Check 'em out - here are the first ten stopwords in the list:\n",
    "\n",
    "```python\n",
    "stopwords[:10]\n",
    "```\n",
    "\n",
    "```text\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
    "```\n",
    "\n",
    "These words don't have any emotional context, descriptive information, or tell you anything about the subject. So you'll want to filter these out, and others like them, so they don't clutter up your analysis.\n",
    "\n",
    "```python\n",
    "wordsWithoutStops = []\n",
    "for word in words:\n",
    "    if word not in stopwords:\n",
    "        wordsWithoutStops.append(word)\n",
    "```\n",
    "\n",
    "Feel familiar? Yeah, it's another for loop. This makes a dictionary named `wordsWithoutStops` that filters out all the stopwords, so only the good stuff is left.  Kind of like straining your jam to remove the seeds, so you're only left with yummy jelly.\n",
    "\n",
    "Want to see the first five words, without stops? Easy - just call:\n",
    "\n",
    "```python\n",
    "wordsWithoutStops[:5]\n",
    "```\n",
    "\n",
    "And you receive back:\n",
    "\n",
    "```text\n",
    "['count', 'monte', 'cristo', 'alexandre', 'dumas']\n",
    "```\n",
    "\n",
    "Now that gets to the more important stuff, doesn't it?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3f800-0a14-4d13-8c32-390965c537ca",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 8 - Count and Plot Words<a class=\"anchor\" id=\"DS106L10_page_8\"></a>\n",
    "\n",
    "[Back to Top](#DS106L10_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597e4b4-ce22-448d-843e-eab5ba74a9c4",
   "metadata": {},
   "source": [
    "# Count and Plot Words\n",
    "\n",
    "You have successfully brought in data from a website with a get request, made it usable with BeautifulSoup, tokenized it, and cleaned it.  Now you can plot it.\n",
    "\n",
    "```python\n",
    "sns.set()\n",
    "frequencyDis = nltk.FreqDist(wordsWithoutStops)\n",
    "frequencyDis.plot(25)\n",
    "```\n",
    "\n",
    "Line two will get a frequency count of all the words in the `wordsWithoutStops` dictionary, using the function `nltk.FreqDist()`.  Then you can easily plot it with the `.plot()` command from `matplotlib`.  The `25` in parentheses says that you're only going to plot the top 25 words, though of course you could change that to whatever you like.\n",
    "\n",
    "Here is the resulting plot:\n",
    "\n",
    "![A graph showing the frequency count of the top twenty five words in the words without stops dictionary. Each word is listed on the x axis, which is labeled sample. The y axis is labeled counts and runs from five hundred to three thousand five hundred in increments of five hundred. A line begins at the top left of the graph at three thousand five hundred for the word said, then sharply decreases to one thousand five hundred for the word one, and then decreases more slowly across the remaining twenty three words.](Media/NLP1.png)\n",
    "\n",
    "When looking at this graph, it is altogether possible that a few more stopwords could have been removed, like \"would\" or \"could.\" However, these can sometimes be helpful.  For instance, when looking at product reviews, wouldn't it be helpful to see if they \"would\" recommend the product to a friend or \"wouldn't?\" You can see how these words in a novel don't help, but how they might help in other contexts.  You do get a sense for the main characters in this novel, though - so this analysis has really gotten to the heart of the issue.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36943bcd-7e16-4af1-97d3-c09e81ae5e99",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 9 - Key Terms<a class=\"anchor\" id=\"DS106L10_page_9\"></a>\n",
    "\n",
    "[Back to Top](#DS106L10_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2cdde-9447-49b5-8936-17d80f46b45e",
   "metadata": {},
   "source": [
    "# Key Terms\n",
    "\n",
    "Below is a list and short description of the important keywords learned in this lesson. Please read through and go back and review any concepts you do not fully understand. Great Work!\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Natural Language Processing</td>\n",
    "        <td>The process of making raw text data useful.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Soup</td>\n",
    "        <td>Webpage data converted to a usable format.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Token</td>\n",
    "        <td>A part of language, such as a word or a sentence.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>Stopwords</td>\n",
    "        <td>Parts of speech that do not convey meaning.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## Key Python Packages\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>requests</td>\n",
    "        <td>Allows you to pull data from a webpage.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>BeautifulSoup</td>\n",
    "        <td>Turns raw web data into something you can actually use.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>RegexpTokenizer</td>\n",
    "        <td>Allows you to split your data into meaningful chunks.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>nltk</td>\n",
    "        <td>Natural language processing package.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## Key Python Code\n",
    "\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>Keyword</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>requests.get()</td>\n",
    "        <td>Function to pull data from a webpage.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>type()</td>\n",
    "        <td>Function to determine the format of your data.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>BeautifulSoup()</td>\n",
    "        <td>Tags the raw data in HTML to make soup.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>RegexpTokenizer()</td>\n",
    "        <td>Specifies the break points in your data.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>tokenize()</td>\n",
    "        <td>Breaks your data down into tokens.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>.lower()</td>\n",
    "        <td>Makes all the words lowercase.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>nltk.corpus.stopwords.words()</td>\n",
    "        <td>Retrieves a list of stopwords in the language specified in the argument.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-weight: bold;\" nowrap>nltk.FreqDist()</td>\n",
    "        <td>Counts all the words in the data and creates a frequency distribution with them.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d02d5-04fa-4220-9637-d02c3b32dce5",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "# Page 10 - Lesson 5 Hands-On<a class=\"anchor\" id=\"DS106L10_page_10\"></a>\n",
    "\n",
    "[Back to Top](#DS106L10_toc)\n",
    "\n",
    "<hr style=\"height:10px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea829d-fa6f-4b15-bfe7-5453b79b1f63",
   "metadata": {},
   "source": [
    "\n",
    "This Hands-­On **will** be graded, so make sure you complete each part. When you are done, please submit one document with all of your findings for grading.\n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Do not submit your project until you have completed all requirements, as you will not be able to resubmit.</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Natural Language Processing Hands On\n",
    "\n",
    "For this hands-on, you will be using *Alice's Adventures in Wonderland* by Lewis Carroll to practice your newfound NLP skills. The book can be found **[here](https://www.gutenberg.org/files/11/11-h/11-h.htm)**. Follow the process you used on *The Count of Monte Cristo* to create a graphic of the most frequently used words in *Alice's Adventures in Wonderland*.\n",
    "\n",
    "Please attach a Jupyter Notebook with your code, your graphic, and your conclusions.\n",
    "\n",
    "<div class=\"panel panel-danger\">\n",
    "    <div class=\"panel-heading\">\n",
    "        <h3 class=\"panel-title\">Caution!</h3>\n",
    "    </div>\n",
    "    <div class=\"panel-body\">\n",
    "        <p>Be sure to zip and submit your entire directory when finished!</p>\n",
    "    </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
